https://arxiv.org/pdf/1712.04440.pdf

data distillation: towards omni-supervised learning

omni supervised learning: a special regime of semi-supervised learning in which learner exploits all labeled data plus internet -scale sources of unlabeled data.
    - lower bounded by performance of labeled data set
    - to exploit omni-supervised learning we  use data distillation, a method that ensembles predictions from multiple transformations of unlabeled data using a single model to generate new training annotations.
    - in cases of object detection, models trained with data distillation surpass the performance of using labeled data ftom the COCO dataset alone.
    - train model on labeled data, then create new annotated data, using the model and retrain model on labeled and newly annotated data. 
        - judge success based on how much it passes fully supervised baseline.
    - after creating annotated data we then perform a transformation on it(flipping, scaling etc).
        - we distill the knowledge of a single model run on multiple transformed copies of unlabeled data
    - data distillation: self training method that makes predictions on unlabeled data and model updates based on this.
    - trained a mask R-CNN model on original COCO set and another large unlabeled set.
data distillation 4 steps:
    1. train model on manually labeled data as in supervised learning
    2. applying the trained model to multiple transofrmations of unlabeled data
    3. converting the predictions on the unlabeled data into labels by ensembling the multiple predictions.
    4. retrain the model on labeled data and newly annotated unlabeled data predictions

multi-transform inference:
    a way to increase the accuracy of visual recognition model. is to apply the model to multiple transformations of the input and agregate the results.
        ex. (multiple crops of image, multiple image scales)
    - data distillation applies multi-transform inference on large unlabeled sets.

aggregating the results of multiple-transform inference, it often has a single prediction that is superior to any of the model's predictions after a single transform.

we ensemble or aggregate the predictions form multi-transform inference in a way that generates "hard labels"

the a student model "the same as the previous model or a new model" can be used to improve the model, is trained with the union data set.
    training on union data set: 
        - no change to the loss function
        - each minibatch containes labaled and automatically labeled data
        - training schedule is lengthed to take advantage of new data.

MASK R CNN:
    stage 1: RPN - region proposal network

For geometric transformations, if
the prediction is a geometric quantity (e.g., coordinates of a
keypoint), then the inverse transformation must be applied
to each prediction before they are merged.

2 popular transformations:
    - scaling and horizontal flipping
    - resize unlabeld data to predefined set of scales

average out the probability of each ROI for each transform we then take the argmax 

we use prediction detection score  as a proxy for prediction quality and generate annotations only from those above a certain threshold.
    -“the average number of annotated instances per unlabeled image” roughly equal to “the average number of instances per labeled image”

- each minibatch is randomly sampled to have a ration of 6:4 labeled to annotated, increase iterations to account for extra newly labeled samples
- learning rate starts at .02 then divides by 10 at 70% and 90%
- student model can be retrained from initial weights, retraining from initial weights is often better performance

multi-transforms on test data improves results.
keypoint head?
