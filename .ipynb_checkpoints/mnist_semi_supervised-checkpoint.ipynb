{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import operator, datetime, math, gc, random\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "pd.options.display.max_rows = 800\n",
    "pd.options.display.max_columns = 800\n",
    "\n",
    "df = pd.read_csv(\"./train.csv\", engine=\"c\", sep=\",\", header=None, skiprows=1)\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parses labels from dataset and normalizes features between 0 and 255 \n",
    "# changed labels to list for consistentcy/ease later\n",
    "\n",
    "def parse_labels_and_features(dataset):\n",
    "    labels = dataset[0].tolist()\n",
    "    features = dataset.loc[:,1:784]\n",
    "    # Scale the data to [0, 1] by dividing out the max value, 255.\n",
    "    features = features / 255.0\n",
    "    return labels, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 460,\n",
       "         0: 425,\n",
       "         4: 409,\n",
       "         7: 427,\n",
       "         3: 386,\n",
       "         5: 398,\n",
       "         8: 404,\n",
       "         9: 401,\n",
       "         2: 454,\n",
       "         6: 436})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting data into training/validation/unlabeled and reshape data\n",
    "\n",
    "labeled_df = df.head(8400)\n",
    "\n",
    "training_targets, training_examples = parse_labels_and_features(labeled_df.head(4200))\n",
    "validation_targets, validation_examples = parse_labels_and_features(labeled_df.tail(4200))\n",
    "unlabeled_targets, unlabeled_examples = parse_labels_and_features(df.tail(33600))\n",
    "training_examples = training_examples.values.reshape((4200, 28, 28, 1))\n",
    "validation_examples = validation_examples.values.reshape((4200, 28, 28, 1))\n",
    "unlabeled_examples = unlabeled_examples.values.reshape((33600, 28, 28, 1))\n",
    "\n",
    "# validation (test) pipeline\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (validation_examples, validation_targets)).batch(32)\n",
    "\n",
    "# distribution of labels in training dataset, try to maintain similar distribution\n",
    "Counter(training_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity check to display a random number \n",
    "\n",
    "rand_example = np.random.choice(9)\n",
    "_, ax = plt.subplots()\n",
    "ax.matshow(training_examples[rand_example].reshape(28, 28))\n",
    "ax.set_title(\"Label: %i\" % training_targets[rand_example])\n",
    "ax.grid(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform functions used by student model\n",
    "\n",
    "def rotate90(images, k=1):\n",
    "    images_shape = images.shape\n",
    "    if len(images_shape) > 4:\n",
    "        ValueError(\"'image' must have either 3 or 4 dimensions, \"\n",
    "                   \"received `{}`.\".format(images_shape))\n",
    "    if len(images_shape) == 4:\n",
    "        return tf.map_fn(lambda img: tf.image.rot90(img, k), images)\n",
    "    return tf.image.rot90(images, k)\n",
    "\n",
    "def crop(images, central_fraction=0.9):\n",
    "    return tf.image.resize(tf.map_fn(lambda img: tf.image.central_crop(img, central_fraction), images), [28, 28])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# techer and student models\n",
    "# TO DO: restructure and possibly add dropout/l1 or l2 normalization\n",
    "\n",
    "class TeacherModel(Model):\n",
    "    def __init__(self):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(128, activation='relu')\n",
    "        self.d2 = Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "# self.conv1 = self.conv2, but was given an error about \n",
    "# creating a new variable on non-initial call when I reused...\n",
    "# TO DO: preprocess transformations concurrently in ETL process\n",
    "class StudentModel(Model):\n",
    "    def __init__(self):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "        self.conv2 = Conv2D(32, 3, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(128, activation='relu')\n",
    "        self.d2 = Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        a = self.conv1(crop(x))\n",
    "        a = self.flatten(a)\n",
    "        a = self.d1(a)\n",
    "        a = self.d2(a)\n",
    "        b = self.conv2(rotate90(x))\n",
    "        b = self.flatten(b)\n",
    "        b = self.d1(b)\n",
    "        b = self.d2(b)\n",
    "        return (b+a)/2\n",
    "\n",
    "teacher_model = TeacherModel()\n",
    "student_model = StudentModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer & loss/accuracy metrics\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')\n",
    "\n",
    "# Paths for tensorboard logs \n",
    "path = '/Users/brianbixby/Desktop/robolab/ai_ml_project/mnist_semi_supervised/logs/gradient_tape/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "teacher_train_summary_writer = tf.summary.create_file_writer(path + '/teacher_train')\n",
    "teacher_test_summary_writer = tf.summary.create_file_writer(path + '/teacher_test')\n",
    "student_train_summary_writer = tf.summary.create_file_writer(path + '/student_train')\n",
    "student_test_summary_writer = tf.summary.create_file_writer(path + '/student_test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error was given when I included @tf.function declarations\n",
    "\n",
    "# @tf.function\n",
    "def train_step(model, optimizer, x_train, y_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x_train)\n",
    "        loss = loss_object(y_train, predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(y_train, predictions)\n",
    "\n",
    "# @tf.function\n",
    "def test_step(model, x_test, y_test):\n",
    "    predictions = model(x_test)\n",
    "    loss = loss_object(y_test, predictions)\n",
    "    test_loss(loss)\n",
    "    test_accuracy(y_test, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes prediction probability for unlabaleled examples,\n",
    "# if the probability is above a certain threshold we anontate it and will train on it\n",
    "\n",
    "def create_unlabeled_predictions(model):\n",
    "    global unlabeled_examples, unlabeled_targets\n",
    "    unlabeled_predictions = model(unlabeled_examples)\n",
    "    unlabeled_training_targets,unlabeled_training_examples,new_unlabeled_examples,new_unlabeled_targets,indexes = [],[],[],[],[]\n",
    "    i,k = 0,0\n",
    "    for pred in unlabeled_predictions:\n",
    "        index, value = max(enumerate(pred), key=operator.itemgetter(1))\n",
    "        if value > .99999:\n",
    "            unlabeled_training_targets.append(index)\n",
    "            unlabeled_training_examples.append(unlabeled_examples[i])\n",
    "            indexes.append(index)\n",
    "            k+=1\n",
    "        else:\n",
    "            new_unlabeled_examples.append(unlabeled_examples[i])\n",
    "            new_unlabeled_targets.append(unlabeled_targets[i])\n",
    "        i+=1\n",
    "    print(\"targets newly labeled: \", len(indexes), \"unlabeled targets left: \", len(new_unlabeled_targets))\n",
    "    unlabeled_examples = np.array(new_unlabeled_examples).reshape((len(new_unlabeled_targets), 28, 28, 1))\n",
    "    unlabeled_targets = new_unlabeled_targets\n",
    "    return np.array(unlabeled_training_examples).reshape((k, 28, 28, 1)), unlabeled_training_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that keeps the 60%/40% original labeled data to annontated data ratio while adding newly annontated data\n",
    "\n",
    "def combine_training_labeled():\n",
    "    global labeled_training_targets,new_labeled_training_targets,training_targets_copy,labeled_training_examples,new_labeled_training_examples,training_examples_copy,training_targets,training_examples\n",
    "    multiplier = math.ceil(1.5*len(new_labeled_training_targets))\n",
    "    training_examples_copy = training_examples_copy.tolist()\n",
    "    \n",
    "    training_zip = list(zip(training_examples, training_targets))\n",
    "    random.shuffle(training_zip)\n",
    "    training_examples, training_targets = zip(*training_zip)\n",
    "    \n",
    "    while multiplier > 4200:\n",
    "        training_targets_copy.extend(training_targets)\n",
    "        training_examples_copy.extend(training_examples)\n",
    "        multiplier -= 4200\n",
    "    training_targets_copy.extend(training_targets[0:multiplier])\n",
    "    training_examples_copy.extend(training_examples[0:multiplier])\n",
    "        \n",
    "    training_targets_copy.extend(new_labeled_training_targets)\n",
    "    labeled_training_targets.extend(new_labeled_training_targets)\n",
    "    training_examples_copy.extend(new_labeled_training_examples.tolist())\n",
    "    new_labeled_training_targets = []\n",
    "    training_examples_copy = np.array(training_examples_copy).reshape((len(training_targets_copy), 28, 28, 1))\n",
    "    labeled_training_examples = labeled_training_examples.tolist()\n",
    "    labeled_training_examples.extend(new_labeled_training_examples)\n",
    "    labeled_training_examples = np.array(labeled_training_examples).reshape((len(labeled_training_targets), 28, 28, 1))\n",
    "    new_labeled_training_examples = np.empty(0)\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0709 21:48:17.049209 140736145658752 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5069196224212646, Accuracy: 84.54762268066406, Test Loss: 0.3005865514278412, Test Accuracy: 90.54762268066406\n",
      "Epoch 2, Loss: 0.1552024632692337, Accuracy: 95.66666412353516, Test Loss: 0.26130762696266174, Test Accuracy: 91.73809051513672\n",
      "Epoch 3, Loss: 0.059187281876802444, Accuracy: 98.57142639160156, Test Loss: 0.21867308020591736, Test Accuracy: 93.66666412353516\n",
      "Epoch 4, Loss: 0.02435634657740593, Accuracy: 99.5714340209961, Test Loss: 0.18511658906936646, Test Accuracy: 95.11904907226562\n",
      "Epoch 5, Loss: 0.011526469141244888, Accuracy: 99.83333587646484, Test Loss: 0.20559419691562653, Test Accuracy: 94.95237731933594\n",
      "targets newly labeled:  9044 unlabeled targets left:  24556\n",
      "Epoch 1, Loss: 0.11733382195234299, Accuracy: 96.5572509765625, Test Loss: 0.19853492081165314, Test Accuracy: 94.4285659790039\n",
      "Epoch 2, Loss: 0.011989424005150795, Accuracy: 99.70532989501953, Test Loss: 0.2138909250497818, Test Accuracy: 95.21428680419922\n",
      "Epoch 3, Loss: 0.007242584601044655, Accuracy: 99.76128387451172, Test Loss: 0.23199737071990967, Test Accuracy: 95.26190948486328\n",
      "Epoch 4, Loss: 0.006723188795149326, Accuracy: 99.76874542236328, Test Loss: 0.24476851522922516, Test Accuracy: 95.21428680419922\n",
      "Epoch 5, Loss: 0.015855945646762848, Accuracy: 99.65684509277344, Test Loss: 0.20571860671043396, Test Accuracy: 95.66666412353516\n",
      "targets newly labeled:  11552 unlabeled targets left:  13004\n",
      "Epoch 1, Loss: 0.034589435905218124, Accuracy: 99.01058959960938, Test Loss: 0.21343520283699036, Test Accuracy: 95.71427917480469\n",
      "Epoch 2, Loss: 9.70439359662123e-05, Accuracy: 100.0, Test Loss: 0.2283666580915451, Test Accuracy: 95.83332824707031\n",
      "Epoch 3, Loss: 2.5003409973578528e-05, Accuracy: 100.0, Test Loss: 0.24801786243915558, Test Accuracy: 95.85714721679688\n",
      "Epoch 4, Loss: 9.12775703909574e-06, Accuracy: 100.0, Test Loss: 0.2637573480606079, Test Accuracy: 95.88095092773438\n",
      "Epoch 5, Loss: 3.900291630998254e-06, Accuracy: 100.0, Test Loss: 0.27770569920539856, Test Accuracy: 95.85714721679688\n",
      "targets newly labeled:  6907 unlabeled targets left:  6097\n",
      "Epoch 1, Loss: 0.043247830122709274, Accuracy: 98.65264892578125, Test Loss: 0.19989146292209625, Test Accuracy: 95.69047546386719\n",
      "Epoch 2, Loss: 0.004288529045879841, Accuracy: 99.78617858886719, Test Loss: 0.23154807090759277, Test Accuracy: 95.76190948486328\n",
      "Epoch 3, Loss: 0.005201103165745735, Accuracy: 99.82181549072266, Test Loss: 0.21877259016036987, Test Accuracy: 96.16666412353516\n",
      "Epoch 4, Loss: 0.003144862363114953, Accuracy: 99.89856719970703, Test Loss: 0.2180856615304947, Test Accuracy: 95.85714721679688\n",
      "Epoch 5, Loss: 0.0021924907341599464, Accuracy: 99.93695068359375, Test Loss: 0.23727284371852875, Test Accuracy: 96.42857360839844\n",
      "targets newly labeled:  1379 unlabeled targets left:  4718\n",
      "Epoch 1, Loss: 0.028372270986437798, Accuracy: 99.20032501220703, Test Loss: 0.18696214258670807, Test Accuracy: 96.0952377319336\n",
      "Epoch 2, Loss: 0.0018292961176484823, Accuracy: 99.94241333007812, Test Loss: 0.20591115951538086, Test Accuracy: 96.07142639160156\n",
      "Epoch 3, Loss: 0.0010266393655911088, Accuracy: 99.97382354736328, Test Loss: 0.2164088487625122, Test Accuracy: 96.47618865966797\n",
      "Epoch 4, Loss: 0.0017172337975353003, Accuracy: 99.95157623291016, Test Loss: 0.252265602350235, Test Accuracy: 96.14286041259766\n",
      "Epoch 5, Loss: 0.0002492681087460369, Accuracy: 99.99476623535156, Test Loss: 0.2413942664861679, Test Accuracy: 96.52381134033203\n",
      "targets newly labeled:  1124 unlabeled targets left:  3594\n",
      "Epoch 1, Loss: 0.045408137142658234, Accuracy: 98.75656127929688, Test Loss: 0.18675357103347778, Test Accuracy: 96.04762268066406\n",
      "Epoch 2, Loss: 0.003396104322746396, Accuracy: 99.90027618408203, Test Loss: 0.19606687128543854, Test Accuracy: 96.11904907226562\n",
      "Epoch 3, Loss: 0.002771907951682806, Accuracy: 99.94698333740234, Test Loss: 0.19234976172447205, Test Accuracy: 96.45237731933594\n",
      "Epoch 4, Loss: 0.0019932726863771677, Accuracy: 99.93940734863281, Test Loss: 0.21977783739566803, Test Accuracy: 96.71428680419922\n",
      "Epoch 5, Loss: 0.002291705459356308, Accuracy: 99.9520263671875, Test Loss: 0.23933733999729156, Test Accuracy: 96.52381134033203\n",
      "targets newly labeled:  457 unlabeled targets left:  3137\n",
      "Epoch 1, Loss: 0.028709961101412773, Accuracy: 99.16250610351562, Test Loss: 0.22515493631362915, Test Accuracy: 95.78571319580078\n",
      "Epoch 2, Loss: 0.0025099879130721092, Accuracy: 99.92035675048828, Test Loss: 0.21533171832561493, Test Accuracy: 96.33332824707031\n",
      "Epoch 3, Loss: 0.0014980875421315432, Accuracy: 99.94275665283203, Test Loss: 0.2536311149597168, Test Accuracy: 95.71427917480469\n",
      "Epoch 4, Loss: 0.0016456929733976722, Accuracy: 99.95271301269531, Test Loss: 0.2759764492511749, Test Accuracy: 95.71427917480469\n",
      "Epoch 5, Loss: 0.0005889448802918196, Accuracy: 99.98506927490234, Test Loss: 0.25098729133605957, Test Accuracy: 96.47618865966797\n",
      "targets newly labeled:  519 unlabeled targets left:  2618\n",
      "Epoch 1, Loss: 0.04742918908596039, Accuracy: 98.63697814941406, Test Loss: 0.17680545151233673, Test Accuracy: 96.45237731933594\n",
      "Epoch 2, Loss: 0.004775200039148331, Accuracy: 99.83589935302734, Test Loss: 0.1706991195678711, Test Accuracy: 96.5\n",
      "Epoch 3, Loss: 0.003861947450786829, Accuracy: 99.86284637451172, Test Loss: 0.19296704232692719, Test Accuracy: 96.64286041259766\n",
      "Epoch 4, Loss: 0.002607554430142045, Accuracy: 99.92652130126953, Test Loss: 0.21787956357002258, Test Accuracy: 96.47618865966797\n",
      "Epoch 5, Loss: 0.0027745929546654224, Accuracy: 99.91917419433594, Test Loss: 0.23078206181526184, Test Accuracy: 96.69047546386719\n"
     ]
    }
   ],
   "source": [
    "EPOCHS,XX = 5,2\n",
    "labeled_training_targets,new_labeled_training_targets = [],[]\n",
    "labeled_training_examples,new_labeled_training_examples = np.empty(0),np.empty(0)\n",
    "training_examples_copy,training_targets_copy = training_examples,list(training_targets)\n",
    "training_examples = training_examples.tolist()\n",
    "\n",
    "# quits out of while when less than 1000 unlabeled examples are labeled\n",
    "while (XX):\n",
    "    gc.collect()\n",
    "    if XX != 2:    \n",
    "        new_labeled_training_examples, new_labeled_training_targets = create_unlabeled_predictions(student_model)\n",
    "        combine_training_labeled()\n",
    "    XX = 1 if len(new_labeled_training_targets) > 1000 else 0\n",
    "    teacher_model = TeacherModel()\n",
    "    train_teacher_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (training_examples_copy, training_targets_copy)).shuffle(len(training_targets_copy)).batch(32)\n",
    "    for epoch in range(EPOCHS):\n",
    "        for (x_train, y_train) in train_teacher_ds:\n",
    "            train_step(teacher_model, optimizer, x_train, y_train)\n",
    "        with teacher_train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "        for (x_test, y_test) in test_ds:\n",
    "            test_step(teacher_model, x_test, y_test)\n",
    "        with teacher_test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "        template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "        print (template.format(epoch+1,train_loss.result(),train_accuracy.result()*100,test_loss.result(),test_accuracy.result()*100))\n",
    "        # Reset metrics every epoch\n",
    "        train_loss.reset_states(),test_loss.reset_states(),train_accuracy.reset_states(),test_accuracy.reset_states()\n",
    "\n",
    "    new_labeled_training_examples, new_labeled_training_targets = create_unlabeled_predictions(teacher_model)\n",
    "    XX = 1 if len(new_labeled_training_targets) > 1000 else XX\n",
    "    combine_training_labeled()\n",
    "    student_model = StudentModel()\n",
    "    train_student_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (training_examples_copy, training_targets_copy)).shuffle(len(training_targets_copy)).batch(32)\n",
    "    for epoch in range(EPOCHS):\n",
    "        for (x_train, y_train) in train_student_ds:\n",
    "            train_step(student_model, optimizer, x_train, y_train)\n",
    "        with student_train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "        for (x_test, y_test) in test_ds:\n",
    "            test_step(student_model, x_test, y_test)\n",
    "        with student_test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "        template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "        print (template.format(epoch+1,train_loss.result(),train_accuracy.result()*100,test_loss.result(),test_accuracy.result()*100))\n",
    "        # Reset metrics every epoch\n",
    "        train_loss.reset_states(),test_loss.reset_states(),train_accuracy.reset_states(),test_accuracy.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# student_model.save('/Users/brianbixby/Desktop/robolab/ai_ml_project/mnist_semi_supervised/student_model.h5')\n",
    "\n",
    "# # Recreate the exact same model purely from the file\n",
    "# new_model = keras.models.load_model('path_to_my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
